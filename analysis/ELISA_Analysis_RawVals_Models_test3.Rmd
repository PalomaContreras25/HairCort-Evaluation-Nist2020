---
title: "ELISA_models"
author: "Paloma"
date: "2024-10-23"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---

# Intro

To explore the effects of each variable more systematically, I run multiple models and compared them using AIC Akakikes' coefficient. I removed samples with a binding over 80% or under 20%.

```{r libraries, echo = FALSE, warning=FALSE, message=FALSE}
# Install libraries
library(dplyr)
library(knitr)
library(ggplot2)
library(RColorBrewer)
library(stats)
library(coefplot)
library(arm)
library(bbmle)

```

Overview of Model Types: Introduce each model with a summary of its purpose.
Model Fitting and Diagnostics: Clearly separate code for each model and add explanations.
Model Comparison: Summarize key findings from model diagnostics.

```{r coef tab function, echo = FALSE}

coeftab <- function(... ,
                    se = FALSE ,
                    se.inside = FALSE ,
                    digits = 4) {
 
    # coeftab - pretty table of coefficients for comparing models
    # works for lm() and a few other object types (e.g., most results of mle2())
    # tends to break when R constructs dummy variables from factors
    # can be avoided by explicitly making dummy variables  before fitting
    
    # initial code written by Richard McElreath (2009)
    # minor updates by Andrew Marshall (2019)
    
    # se = TRUE outputs standard errors
    # se.inside = TRUE prints standard errors in 
    #             parentheses in same column as estimates
    
    # retrieve list of models
    L <- list(...)
    if (is.list(L[[1]]) && length(L) == 1)
        L <- L[[1]]
    
    # retrieve model names from function call
    mnames <- match.call()
    mnames <- as.character(mnames)[2:(length(L) + 1)]
    
    # count number of unique parameters
    param.names <- {
    }
    for (i in 1:length(L)) {
        c.names <- names(coef(L[[i]]))
        param.names <- unique(c(param.names , c.names))
    }
    # columns for standard errors
    if (se == TRUE && se.inside == FALSE) {
        for (i in 1:length(L)) {
            kse.names <- paste(names(coef(L[[i]])) , ".se" , sep = "")
            param.names <- unique(c(param.names , kse.names))
        }
    }
    
    # make empty table
    nk <- length(param.names)
    d <- matrix(NA , ncol = nk)
    d <- data.frame(d)
    colnames(d) <- c(param.names)
    
    # loop over models and insert values
    for (i in 1:length(L)) {
        klist <- coef(L[[i]])
        for (j in 1:length(klist)) {
            d[i, ][names(klist[j])] <- as.numeric(klist[j])
        }
    }
    # insert standard errors
    if (se == TRUE) {
        for (i in 1:length(L)) {
            kse <- sqrt(diag (vcov(L[[i]])))
            for (j in 1:length(kse)) {
                if (se.inside == FALSE)
                    # own column
                    d[i, ][paste(names(kse)[j], ".se", sep = "")] <-
                        as.numeric(kse[j])
                else
                    # combine with estimate
                    d[i, ][names(kse)[j]] <-
                        paste(
                            formatC(as.real(d[i, ][names(kse)[j]]) , digits = digits) ,
                            " (" ,
                            formatC(as.real(kse[j]) , digits = digits) ,
                            ")" ,
                            sep = ""
                        )
            }
        }
    }
    
    # add model names to rows
    rownames(d) <- mnames
    
    # formatting for parenthetical standard errors
    if (se.inside == TRUE && se == TRUE) {
        comment(d) <-
            "Values in parentheses are quadratic estimate standard errors."
        colnames(d) <- paste(colnames(d) , "(se)")
        for (i in 1:nrow(d)) {
            for (j in 1:ncol(d)) {
                d[i, j] <- ifelse(is.na(d[i, j]) , "" , d[i, j])
            }
        }
    }
    
    # return table
    d
}


```

```{r function to extract coefs, echo = FALSE}

# creating function to extract coeffs
extract_coefs <- function(model, model_name) {
  # Extract summary of the model
  coef_summary <- summary(model)$coefficients
  
  # Create a data frame with term names, estimates, and standard errors
  coef_df <- data.frame(
    term = rownames(coef_summary),
    estimate = coef_summary[, "Estimate"],
    std.error = coef_summary[, "Std. Error"],
    model = model_name  # Add the model name as a new column
  )
  
  # Return the data frame
  return(coef_df)
}
```

```{r qqplot, echo = FALSE}

# load dataset
data <- read.csv("./data/Test3/Data_QC_filtered.csv")

# Since Buffer and spike are not continuous variables, 
# change from numeric to factors
data$Buffer_nl <- ifelse(data$Buffer_nl == 250, 1, 0) # Large
data$Buffer_nl <- as.factor(data$Buffer_nl)
data$Spike <- as.factor(data$Spike)


# remove samples w/ binding higher than 80%
d <- data[is.na(data$Failed_samples), ]
data <- d  

weight <- data$Weight_mg
binding <- data$Binding.Perc
x <- mean(binding)


# Q-Q Plot
qqnorm(binding, pch = 16, col = "gray40")
qqline(binding, col = "orange3", lwd = 1.5)

```


This is the distribution of the data (binding percentage). I am not sure how to describe it, but it does not look very linear. I will test different distributions at another time, but for now, I will run and compare simple models that should allow me to understand which variables have a greater impact on binding percentages.

```{r models dif weights, echo = FALSE, include = FALSE}
mod <- lm(binding ~ weight)
summary(mod)

plot(weight, binding,
     main = "Linear regression ~ weight",
     xlab = "Weight",
     ylab = "binding %",
     pch = 16,
     cex = 1.3, 
     col = "gray40")

abline(mod, col = "orange3", lwd = 1.5)

# 20
d<- data[c(data$Weight_mg >= 20 & data$Weight_mg <= 29.9), ]
weight <- d$Weight_mg
binding <- d$Binding.Perc

# Q-Q Plot
qqnorm(binding, pch = 16, col = "cyan3")
qqline(binding, col = "red", lwd = 1.5)

## Linear regression
mod <- lm(binding ~ weight)
summary(mod)

plot(weight, binding,
     main = "Linear regression, weight 20 - 29.9 mg",
     xlab = "weight",
     ylab = "binding %",
     pch = 8,
     cex = 1, 
     col = "darkgreen")

abline(mod, col = "red", lty = 'dashed')

# 30

## Linear regression, by weight group
d<- data[c(data$Weight_mg > 29.9), ]
weight <- d$Weight_mg
binding <- d$Binding.Perc

# Q-Q Plot
qqnorm(binding, pch = 16, col = "cyan3")
qqline(binding, col = "red", lwd = 1.5)

## Linear regression
mod <- lm(binding ~ weight)
summary(mod)

plot(weight, binding,
     main = "Linear regression, weight over 30 mg",
     xlab = "weight",
     ylab = "binding %",
     pch = 8,
     cex = 1, 
     col = "darkgreen")

abline(mod, col = "red", lty = 'dashed')
```

```{r models dif spike, include = FALSE}
## Linear regression, by spike
## YES
d<- data[c(data$Spike == 1), ]
weight <- d$Weight_mg
binding <- d$Binding.Perc

# Q-Q Plot
qqnorm(binding, pch = 16, col = "cyan3", main = "qqplot, spiked samples")
qqline(binding, col = "red", lwd = 1.5)

## Linear regression
mod <- lm(binding ~ weight)
summary(mod)

plot(weight, binding,
     main = "Linear regression, spiked",
     xlab = "weight",
     ylab = "binding %",
     pch = 16,
     cex = 1, 
     col = "darkgreen")

abline(mod, col = "red", lty = 'dashed')


## Linear regression, by spike
## NO

d<- data[c(data$Spike == 0), ]
weight <- d$Weight_mg
binding <- d$Binding.Perc

# Q-Q Plot
qqnorm(binding, pch = 16, col = "cyan3", main = "qqplot, NONspiked samples")
qqline(binding, col = "red", lwd = 1.5)

## Linear regression
mod <- lm(binding ~ weight)
summary(mod)

plot(weight, binding,
     main = "Linear regression, not spiked",
     xlab = "weight",
     ylab = "binding %",
     pch = 16,
     cex = 1, 
     col = "darkgreen")

abline(mod, col = "red", lwd = 1.5)

```




# Linear models: Description

<table>
<caption><span id="tab:table"> Model description </span></caption>

            Spike   Buffer  Weight    Obs
--------  -----   ------  -------   -------------------------
|  m1  |                     X
|  m2  |  X        
|  m3  |           X
|  m4  |  X                  X
|  m5  |           X         X
|  m6  |  X        X
|  m7  |  X        X         X
|  m8  |           X         X        Spiked samples only
|  m9  |           X         X        Not spiked samples only
</table>

## Creating models

```{r create simple linear models}
binding <- data$Binding.Perc
weight <- data$Weight_mg
spike <- data$Spike
buffer <- data$Buffer_nl

# model 1

m1 <- lm(binding ~ weight)
confint(m1, level = 0.95)

# model 2

m2 <- lm(binding ~ spike)
confint(m2, level = 0.95)

# model 3

m3 <- lm(binding ~ buffer)
confint(m3, level = 0.95)

# model 4

m4 <- lm(binding ~ weight + spike)
confint(m4, level = 0.95)

# model 5 

m5 <- lm(binding ~ weight + buffer)
confint(m5, level = 0.95)

# model 6

m6 <- lm(binding ~ spike + buffer)
confint(m6, level = 0.95)

# model 7

m7 <- lm(binding ~ weight + buffer + spike)
confint(m7, level = 0.95)

# model 8

sp1 <- data[data$Spike == 1,]
sp0 <- data[data$Spike == 0,]

binding <- sp1$Binding.Perc
weight <- sp1$Weight_mg
spike <- sp1$Spike
buffer <- sp1$Buffer_nl

m8 <- lm(binding ~  buffer + weight)
confint(m8, level = 0.95)

# model 9

binding <- sp0$Binding.Perc
weight <- sp0$Weight_mg
spike <- sp0$Spike
buffer <- sp0$Buffer_nl

m9 <- lm(binding ~  buffer + weight)
confint(m9, level = 0.95)

```

## QQ plots

```{r qqplots models, fig.height = 8, fig.width = 7}
model <- list(m1, m2, m3, m4, m5, m6, m7, m8, m9)

par(mfrow = c(3, 3))

for (i in 1:length(model)) {
  # Create a Q-Q plot for the residuals of the i-th model
  qqnorm(residuals(model[[i]]), main = paste("Q-Q Plot, m", i, sep = ""))
  qqline(residuals(model[[i]]), col = "red")
}
```

## Plot residuals vs fitted by model

```{r residuals models, fig.height = 8, fig.width = 7}
par(mfrow = c(3, 3))

plot(m1, which = 1)  
plot(m2, which = 1, main = "m2")  
plot(m3, which = 1, main = "m3")  
plot(m4, which = 1, main = "m4")
plot(m5, which = 1, main = "m5") 
plot(m6, which = 1, main = "m6")
plot(m7, which = 1, main = "m7")
plot(m8, which = 1, main = "m8")
plot(m9, which = 1, main = "m9")
```



# Comparing models 

## (Akakike's information criteria) 

```{r AICc}

# computing bias-adjusted version of AIC (AICc) Akakaike's information criteria table
AICc_compare <-AICtab(m1, m2, m3, m4, m5, m6, m7, m8, m9, 
        base = TRUE,
        weights = TRUE,
        logLik  = TRUE,
        #indicate number of observations
        nobs = 30)
kable(AICc_compare)


```

**Model 7 has the highest weight, a measure of certainty in the model**. However, we need to consider that the distribution of the data is not normal. Perhaps I should try using other distributions (binom, posson, )


```{r include = FALSE}

model_names <- paste("m", 1:9, sep="")
r_values <- 1:9
all_models <- list(m1, m2, m3, m4, m5, m6, m7, m8, m9)
model_info <- c("weight", "spike", "buffer", 
                "weight + spike", 
                "weight + buffer", 
                "spike + buffer", 
                "spike + buffer + weight", 
                "buffer + weight, spiked only",
                "buffer + weight, NOT spiked only")
sum_models <- as.data.frame(r_values, row.names= model_names)
sum_models$info <- model_info
sum_models$res_std_error <- 1:length(model_names)


for (i in 1:length(model_names)) {
    sum_models$r_values[i]      <- summary(all_models[[i]])$adj.r.squared
    sum_models$res_std_error[i] <- summary(all_models[[i]])$sigma
}

```

## R values and std error

```{r}
kable(sum_models[order(sum_models$r_values, decreasing = TRUE), ]) 
```


# Plot regression coefs 
```{r Plot reg coefs, echo = FALSE, message = FALSE, include = FALSE}

coef_df1 <- extract_coefs(m1, "Model 1")
coef_df2 <- extract_coefs(m2, "Model 2")
coef_df3 <- extract_coefs(m3, "Model 3")
coef_df4 <- extract_coefs(m4, "Model 4")
coef_df5 <- extract_coefs(m5, "Model 5")
coef_df6 <- extract_coefs(m6, "Model 6")
coef_df7 <- extract_coefs(m7, "Model 7")
coef_df8 <- extract_coefs(m8, "Model 8")
coef_df9 <- extract_coefs(m9, "Model 9")

# Combine the data frames for plotting
coef_df <- rbind(coef_df1, coef_df2, coef_df3, coef_df4)

ggplot(coef_df, aes(x = term, 
                    y = estimate, 
                    color = model)) +
  geom_point(position = position_dodge(width = 0.5)) +  # Points for the estimates
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, 
                    ymax = estimate + 1.96 * std.error),
                    position = position_dodge(width = 0.5), width = 0.2) +  
  theme_minimal() +
  coord_flip() +  # Flip the coordinates for better readability
  labs(title = "Coefficient Plot for Multiple Models",
       x = "Terms",
       y = "Estimates") +
  geom_hline(yintercept = 0, 
             color = "gray", 
             linetype = "dashed") +  # Gray line at zero
  theme(legend.position = "bottom")

```

### Plot model 1 to 4

```{r Plot model 1 to 4, warning=FALSE, fig.height = 6, fig.width = 6 }

ggplot(coef_df, aes(x = term, 
                    y = estimate, 
                    color = model)) +
  geom_point(position = position_dodge(width = 4)) +  
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, 
                    ymax = estimate + 1.96 * std.error),
                position = position_dodge(width = 0.85), width = 1) + 
  theme_minimal() +
  coord_flip() +  # Flip the coordinates for better readability
  facet_wrap(~ model, ncol = 1) +  # One model per line
  labs(title = "Coefficient Plot for Models 1-4",
       x = "Terms",
       y = "Estimates") +
  theme(legend.position = "none") +
  geom_hline(yintercept = 0, color = "gray", 
             linetype = "dashed") +  # Gray line at zero
  expand_limits(y = c(-58, 58)) +
  theme(
    axis.text.x = element_text(size = 12),        # X-axis text size
    axis.text.y = element_text(size = 12),        # Y-axis text size
    axis.title.x = element_text(size = 14),       # X-axis title size
    axis.title.y = element_text(size = 14),       # Y-axis title size
    plot.title = element_text(size = 16, hjust = 0.5),  
    strip.text = element_text(size = 14)          # Facet label text size
  )
```

### Plot model 5 to 9

```{r Plot model 5 to 9, warning=FALSE, fig.height = 7, fig.width = 6 }
# Combine the data frames for plotting

coef_df <- rbind(coef_df5, coef_df6, coef_df7, coef_df8,coef_df9)

ggplot(coef_df, aes(x = term, 
                    y = estimate, 
                    color = model)) +
  geom_point(position = position_dodge(width = 3)) +  
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, 
                    ymax = estimate + 1.96 * std.error),
                position = position_dodge(width = 0.9), width = 0.85) + 
  theme_minimal() +
  coord_flip() +  # Flip the coordinates for better readability
  facet_wrap(~ model, ncol = 1) +  # One model per line
  labs(title = "Coefficient Plot for Model 5 to 9",
       x = "Terms",
       y = "Estimates") +
  theme(legend.position = "none") +
  geom_hline(yintercept = 0, color = "gray", 
             linetype = "dashed") +  # Gray line at zero
  expand_limits(y = c(-60, 60)) +
  theme(
    axis.text.x = element_text(size = 12),        # X-axis text size
    axis.text.y = element_text(size = 12),        # Y-axis text size
    axis.title.x = element_text(size = 14),       # X-axis title size
    axis.title.y = element_text(size = 14),       # Y-axis title size
    plot.title = element_text(size = 16, hjust = 0.5),  
    strip.text = element_text(size = 14)          
  )



``` 


### Additional info (coefficient table)

```{r coef table, include = TRUE}

# Coef table 
coeftab(m1, m2, m3, m4, m5, m6, m7, m8, m9) -> coeftabs
kable(coeftabs)

```

```{r Models mle2 and dif distributions, include = FALSE}

#scale variable

d2 <- data
d2$y <- data$Binding.Perc/100 

nll_beta <- function(mu, phi) {
  a <- mu * phi
  b <- (1 - mu) * phi
  -sum(dbeta(d2$y, a, b, log = TRUE))
}

# Fit models using mle2

fit <- mle2(nll_beta, start = list(mu = 0.5, phi = 1), data = d2)
summary(fit)

m0n <- mle2(d2$y ~ dnorm(mean = a, sd = sd(d2$y)), start = list(a = mean(d2$y)), data = d2) 

# percent cover as predictor, use normal distribution
mcn <- mle2(d2$y ~ dnorm(mean = a + b * d2$Weight_mg, sd = sd(d2$y)), start = list(a = mean(d2$y), b = 0, s = sd(d2$y)), data = d2)

# scatter plot of 

plot(d2$y ~ d2$Weight_mg, 
     xlab = "Buffer",
     ylab = "% binding",
     col = "salmon",
     pch = 16, 
     las = 1)

#m0n
k <-coef(m0n)
curve(k[1] + 0 * x, 
      from = 0, to = 100, 
      add=T, lwd = 3, 
      col = "black")

#mcn
k <-coef(mcn)
curve(k[1] + k[2] * x, 
      from = 0, to = 100, 
      add=T, lwd = 2, 
      col = "lightgreen", 
      lty = "dashed")

```

# Optimal parameters using model 7

The goal is to run essays that result in a 50% binding. Here I can find the weight that gives 50% binding when spike is 0

```{r Find optimal parameters}
# choose one model (m7: buffer + weight + spike)
coef <- coef(m7)

# Set target binding
target_binding <- 50

# FUNCTION to Solve for weight, assuming spike = 0
# 50% - intercept - (buffer1 * 1) - (spike * 0) / weight  

solve_for_weight <- function(dilution_value, spike_value = 0) {
  (target_binding - coef[1] - coef[3] * dilution_value - coef[4] * spike_value) / coef[2]
}

# Find the weight that gives 50% binding when spike is 0

# dilution = 250
optimal_weight <- solve_for_weight(dilution_value = 1)
optimal_weight

# dilution = 60
optimal_weight <- solve_for_weight(dilution_value = 0)
optimal_weight

# Find the weight that gives 50% binding when spike is 1

# dilution = 250
optimal_weight <- solve_for_weight(dilution_value = 1, spike_value = 1)
optimal_weight

# dilution = 60
optimal_weight <- solve_for_weight(dilution_value = 0, spike_value = 1)
optimal_weight
```

## Visualize estimated optimal parameters

```{r Find optimal parameters2}
# FUNCTION to Solve for weight, assuming spike = 0
# 50% - intercept - (buffer1 * 1) - (spike * 0) / weight  

solve_for_weight <- function(dilution_value, spike_value = 0) {
  (target_binding - coef[1] - coef[3] * dilution_value - coef[4] * spike_value) / coef[2]
}

# Loop over different dilution values to find the optimal weight for 50% binding
# here, dilution 0 means 60uL, and 1 is 250 uL
for (buffer in seq(0, 1, by = 0.1)) {
  optimal_weight <- solve_for_weight(buffer)
  cat("Dilution:", buffer, "-> Optimal Weight for 50% Binding:", optimal_weight, "\n")
}

```

```{r Visualize estimated optimal parameters}
# visualize results
## Buffer = 1, Spike = 0
new_data.b1.s0 <- expand.grid(weight = seq(min(weight), max(weight), 
                                     length.out = 100),
                        buffer = as.factor(1), 
                        spike = as.factor(0))  

# Predict the binding percentage for the new data
new_data.b1.s0$predicted_binding <- predict(m7, 
                                            newdata = new_data.b1.s0)

## Buffer = 1, Spike = 1
new_data.b1.s1 <- expand.grid(weight = seq(min(weight), max(weight), 
                                     length.out = 100),
                        buffer = as.factor(1), 
                        spike = as.factor(1))  

# Predict the binding percentage for the new data
new_data.b1.s1$predicted_binding <- predict(m7, 
                                            newdata = new_data.b1.s1)

## Buffer = 0, Spike = 1

new_data.b0.s1 <- expand.grid(weight = seq(min(weight), 
                                     max(weight), length.out = 100),
                        buffer = as.factor(0), spike = as.factor(1)) 

# Predict the binding percentage for the new data
new_data.b0.s1$predicted_binding <- predict(m7, 
                                            newdata = new_data.b0.s1)

## Buffer = 0, Spike = 0

new_data.b0.s0 <- expand.grid(weight = seq(min(weight), max(weight), length.out = 100),
                        buffer = as.factor(0), spike = as.factor(0))  # Set buffer and spike to a fixed value for simplicity

# Predict the binding percentage for the new data
new_data.b0.s0$predicted_binding <- predict(m7, 
                                            newdata = new_data.b0.s0)

new_data <- rbind(new_data.b0.s0, new_data.b0.s1, new_data.b1.s0, new_data.b1.s1)


# Plot the predicted binding percentage against weight
ggplot(new_data, aes(x = weight, 
                     y = predicted_binding)) +
  geom_line() +
   geom_hline(yintercept = 50, linetype = "dashed", color = "red") +
  labs(x = "Weight",
       y = "Predicted Binding Percentage") +
  facet_grid(buffer ~ spike, 
             labeller = labeller(
                 buffer = c("0" = "60 uL buffer",
                          "1" = "250 uL buffer"),
                 spike = c("0" = "No Spike", 
                           "1" = "Spiked")))


# Plot the predicted binding percentage against weight
ggplot(new_data.b1.s1, aes(x = weight, 
                     y = predicted_binding)) +
  geom_line(color = "gold3") +
  geom_hline(yintercept = 50, 
             linetype = "dashed", 
             color = "red") +  # Highlight 50% binding
  labs(title = "Predicted Binding Percentage vs Weight 
       (dilution = 250, spike = 1)",
       x = "Weight",
       y = "Predicted Binding Percentage")+
  theme_minimal()

# Plot the predicted binding percentage against weight
ggplot(new_data.b0.s1, aes(x = weight, 
                     y = predicted_binding)) +
  geom_line(color = "gold3") +
  geom_hline(yintercept = 50, 
             linetype = "dashed", 
             color = "red") +  # Highlight 50% binding
  labs(title = "Predicted Binding Percentage vs Weight (dilution = 250, spike = 0)",
       x = "Weight",
       y = "Predicted Binding Percentage") +
  theme_minimal()

# Plot the predicted binding percentage against weight
ggplot(new_data.b0.s0, aes(x = weight, 
                     y = predicted_binding)) +
  geom_line(color = "gold3") +
  geom_hline(yintercept = 50, 
             linetype = "dashed", 
             color = "red") +  # Highlight 50% binding
  labs(title = "Predicted Binding Percentage vs Weight (dilution = 60, spike = 0)",
       x = "Weight",
       y = "Predicted Binding Percentage") +
  theme_minimal()
```


```{r echo = FALSE}

#wflow_publish("./analysis/ELISA_models.Rmd")
#wflow_status()

```



