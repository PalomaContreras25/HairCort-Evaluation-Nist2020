---
title: "Results"
author: "Paloma"
date: "2024-10-16"
output: workflowr::wflow_html
editor_options:
  chunk_output_type: console
---
# Introduction

Here I use QCed results from an ELISA plate. All hair samples were obtained from the same person. I tested 3 variables:

- dilution (60 nL vs 250 nL, coded as 0 and 1, respectively)

- weight (11 to 37.1 mg)

- spike (25 nL stock solution (1:10) added to some wells, coded as 0 and 1, meaning not-spiked and spiked)


I removed the samples that have a Coef of variation higher than 15%.

```{r libraries, echo = FALSE, warning = FALSE, message=FALSE}
# Install libraries
library(dplyr)
library(knitr)
library(ggplot2)
library(RColorBrewer)
library(stats)
library(coefplot)
library(arm)
library(bbmle)
```

# Summary of results 

The **figures** below were used to decide that the **optimal parameters** are:

- **Weight** higher than 20, ideally. Binding deviation goes down with higher weight.
- **Dilution** between 60 and 250 ul (higher dilution provides lower coef. of variation, more lower dilutions locate samples closer to 50% binding. An intermedieate value would be best)
- **Spike** Non-spiked samples provide lower binding deviation from 50% (i.e. measures are close to falling outside the curve) 



```{r scatterplot 2, message=FALSE, echo=FALSE, fig.height = 4.5, fig.width = 7}

data <- read.csv(
  "./data/Data_cort_values.csv"
  )

# Scatter plot of Binding Percentage vs Weight

ggplot(data, aes(x = Weight_mg, 
                 y = Binding.Perc, 
                 shape = factor(Buffer_nl))) +
  geom_point(size = 3, color = "gold2") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.4, color = "orange3") +  # Add a trend line
  geom_hline(yintercept = 50, linetype = "dashed", 
             color = "gray", linewidth = 1) +  # Add horizontal line 
  labs(title = "Results separated by spiked/non-spiked",
       x = "Weight (mg)", y = "Binding Percentage",
       shape = "Buffer Amount (nl)") +
  scale_y_continuous(n.breaks = 10) +  
  scale_shape_discrete(labels = c("60 uL", "250 uL")) +
  facet_grid( ~ Spike, labeller = as_labeller(c("0" = "Not spiked", "1" = "Spiked"))) +
  theme_dark()

``` 
Here we see the effect of the spike more clearly: adding a spike may not be necessary unless we have very small samples.


Spiked samples were removed due to inconsistent results.



```{r echo=FALSE, message = FALSE}

# non-spiked samples only
data2 <-data[data$Spike == 0, ]

#two datasets, separated by dilution
data2.06 <- data2[data2$Buffer_ml == 0.06, ]
data2.25 <- data2[data2$Buffer_ml == 0.25, ]

final_conc_pg.mg <- data2$final_conc_pg.mg 
Weight_mg <- data2$Weight_mg
# fit models 
model <- lm(final_conc_pg.mg ~ Weight_mg, 
            data = data2)

# model Buffer = 0.06
model06 <- lm(final_conc_pg.mg ~ Weight_mg, 
              data = data2[data2$Buffer_ml == 0.06,])
r_squared06 <- summary(model06)$r.squared

# model Buffer = 0.25
model25 <- lm(final_conc_pg.mg ~ Weight_mg, 
              data = data2[data2$Buffer_ml == 0.25,])
r_squared25 <- summary(model25)$r.squared

# Calculate residuals
residuals06 <- residuals(model06)
residuals25 <- residuals(model25)

# Quantify residuals
# Mean Absolute Error
mae06 <- mean(abs(residuals06))          
# Standard Deviation of Residuals
std_dev06 <- sd(residuals06)   

# Mean Absolute Error
mae25 <- mean(abs(residuals25))      
# Standard Deviation of Residuals
std_dev25 <- sd(residuals25)                      

# scatterplot

ggplot(data2, aes(y = final_conc_pg.mg, 
                  x = Weight_mg, 
                  color = as.factor(Buffer_ml), 
                  fill = as.factor(Buffer_ml))) +
  geom_point(size = 3) +  
  geom_smooth(method = "lm", 
              color = "gold3", 
              se = TRUE,
              alpha = 0.1) + 
  geom_hline(yintercept = mean(data2$final_conc_pg.mg), 
             color = "gray90",
             linetype = "dashed") +
  geom_hline(yintercept = mean(data2.06$final_conc_pg.mg), 
             color = "lightpink2",
             linetype = "dashed") +
  geom_hline(yintercept = mean(data2.25$final_conc_pg.mg), 
             color = "lightblue3",
             linetype = "dashed") +
  theme_minimal() +  
  labs(
    title = "Final Cort Concentration and Weight
    (Non-spiked only)",
    y = "Final Concentration (pg/mg)",
    x = "Weight (mg)" 
    ) +
  scale_color_discrete(name = "Buffer (ml)")+
  scale_fill_discrete(guide = "none") +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),  
    axis.title = element_text(size = 14),  
    axis.text = element_text(size = 12) 
  ) +
  # Add R^2 annotation
  annotate("text", x = max(data2$Weight_mg) * 0.7, y = min(data2$final_conc_pg.mg) * 1.5,
           label = paste("R² =", round(r_squared06, 3)), size = 5, color = "black") +
  annotate("text", x = max(data2$Weight_mg) * 0.7, y = max(data2$final_conc_pg.mg) * 0.84,
           label = paste("R² =", round(r_squared25, 3)), size = 5, color = "black")

```

# Results

```{r echo = FALSE}

# load dataset
data <- read.csv("./data/Data_QC_filtered.csv")

# Since Buffer and spike are not continuous variables, 
# change from numeric to factors
data$Buffer_nl <- ifelse(data$Buffer_nl == 250, 1, 0) # Large
data$Buffer_nl <- as.factor(data$Buffer_nl)
data$Spike <- as.factor(data$Spike)

# Histogram
hist(data$Binding.Perc, breaks=30, main="Histogram of Binding percentages", xlab="Binding %", col="lightblue4", xlim = range(0:100))
```



The following plots were made considering that having a binding of 50% is ideal. Data points that are over 80% or under 20% are not within the curve, and predictions are less accurate. 


## Binding percentages
### Binding percentage by different variables

```{r scatterplot 1, message =FALSE, echo=FALSE}

# Scatter plot of Binding Percentage vs Weight, 4 groups

ggplot(data, aes(x = Weight_mg, 
                 y = Binding.Perc, 
                 color = factor(Spike), 
                 shape = factor(Buffer_nl))) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.5) +  # Add a trend line
  geom_hline(yintercept = 50, linetype = "dashed", 
             color = "gray", linewidth = 1) +  # Add horizontal line
  labs(title = "Binding Percentage vs Weight, 4 groups",
       x = "Weight (mg)", 
       y = "Binding Percentage",
       color = "Spike Status",  
       shape = "Buffer Amount (nl)") +
  scale_y_continuous(n.breaks = 10) +  
  theme_minimal() +
  # Change the labels in the legend using labs
  scale_color_discrete(labels = c("No Spike", "Spike Added")) +  # Change color legend labels
  scale_shape_discrete(labels = c("60 uL", "250 uL"))  

```

- **Spiked** samples (turquoise) have lower binding, because they have higher levels of cortisol than non spiked (pink) samples. 

- **Dilution**: effect is less clear. We see samples with both 60 uL and 250 uL binding at very high and very low levels. 

- **Trends**: within non-spiked samples with a similar weight and diluted at 60uL (pink circles), we do not obtain consistent binding percentages. However, non-spiked samples with similar weights do obtain similar bindings, and the lines are in the expected direction (higher weight, lower binding), except by a few outliers that would be removed from the analysis anyway (for having binding over 80%)

- **Conclusion**: samples across different weights, **non-spiked, and diluted in 250 uL buffer seem to provide the best results**, particularly if samples **weigh more than 15 mg**. Using less than that may be risky, and in those cases, it may be better to use less buffer to concentrate the samples a bit more. 




### Value distributions by group (boxplots) 

```{r boxplot1, echo = FALSE}
## Boxplot of Binding Percentage by dilution

ggplot(data, aes(x = factor(Spike), 
                 y = Binding.Perc, 
                 fill = factor(Spike))) +
  geom_boxplot() +
  geom_hline(yintercept = 50, linetype = "dashed", 
             color = "gray", linewidth = 1) + 
  labs(title = "Binding Percentage by Dilution",
       x = "", 
       y = "Bindings Percentage") +
  scale_y_continuous(n.breaks = 10) +  
  theme(legend.position = "none") +
  facet_wrap(~ Buffer_nl,  
             labeller = as_labeller(c("0" = "60 uL", 
                                      "1" = "250 uL"))) +
    scale_x_discrete(labels = c("Not spiked", "Spiked")) 
```

Here we also see that the impact of the spike on the values is larger than the impact of using a different dilution


## Coef. of variation percentage

The coefficient of variation or CV is a standardized measure of the difference between duplicates (same sample, same weight, same dilution, same everything). Some variables may make duplicates more variable, so this is what will be tested below.  

### Coef. of variation by group
```{r CVbyGroup, echo=FALSE}
# Boxplot of CV Percentage vs Buffer & Spike

ggplot(data, aes(x = factor(Spike), 
                 y = CV.Perc, 
                 fill = factor(Spike))) +
  geom_boxplot() +
  labs(title = "Coef. of variation by dilution and spike",
       x = "", 
       y = "Coef. of variation (%)") +
  scale_y_continuous(n.breaks = 10) +  
  theme(legend.position = "none") +
  facet_wrap(~ Buffer_nl,  
             labeller = as_labeller(c("0" = "60 uL", 
                                      "1" = "250 uL"))) +
    scale_x_discrete(labels = c("Not spiked", "Spiked")) 
```

**Conclusion** diluting the sample less seems to lead to higher differences between duplicates, which is something we want to avoid. We also see less variation for the group of spiked samples, with the lowest average of the four groups. Yet, we also must note that the spiked, 250 uL group has only 6 samples, as we see on the table below.

<table>
<caption><span id="tab:table">Num_of_samples </span></caption>

Dilution:              No spike       Spiked
------              -------------  ---------
60 uL                   7          7
250 uL                    12          6
Total: 32 samples
</table>

### Coef. of variation by different variables

```{r message =FALSE, echo=FALSE}

# Scatter plot of CV Percentage vs Weight, 4 groups

ggplot(data, aes(x = Weight_mg, 
                 y = CV.Perc, 
                 color = factor(Spike), 
                 shape = factor(Buffer_nl))) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.5) +  
  labs(title = "Coef. of variation Percentage vs Weight, 4 groups",
       x = "Weight (mg)", 
       y = "Coef. of variation (%)",
       color = "Spike Status",  
       shape = "Buffer Amount (nl)") +
  scale_y_continuous(n.breaks = 10) +  
  theme_minimal() +
  # Change the labels in the legend using labs
  scale_color_discrete(labels = c("No Spike", "Spike Added")) +  # Change color legend labels
  scale_shape_discrete(labels = c("60 uL", "250 uL"))  

```

**Lower CV** is seen in spiked + 250 uL group, particularly for samples with low weight. Yet, non spiked, diluted in 250uL samples have very low CV if weight is over 30. 

## Deviation from 50% binding

Here I calculate a "binding" deviation score, to have a better idea of the "distance" between the values obtained and what I should aim for: 50% binding. Here an example of how this score works:

```{r calc binding deviation, echo=FALSE}

data$Binding_deviation <- abs(data$Binding.Perc - 50)
sorted_data <- data[order(data$Binding_deviation), ]

# View top results (closest to 50% Binding Percentage)
kable(head(sorted_data[,c("Sample", "Binding.Perc", "Binding_deviation")]))

```


```{r plot_deviation, message=FALSE, echo = FALSE, include = TRUE}

# Scatter plot of Binding Deviation vs Weight, 4 groups

ggplot(data, aes(x = Weight_mg, 
                 y = Binding_deviation, 
                 color = factor(Spike), 
                 shape = factor(Buffer_nl))) +
  geom_point(size = 3) +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.5) +  
  labs(title = "Binding Deviation vs Weight, 4 groups",
       x = "Weight (mg)", 
       y = "Binding Deviation",
       color = "Spike Status",  
       shape = "Buffer Amount (nl)") +
  scale_y_continuous(n.breaks = 10) +  
  theme_minimal() +
  # Change the labels in the legend using labs
  scale_color_discrete(labels = c("No Spike", "Spike Added")) +  # Change color legend labels
  scale_shape_discrete(labels = c("60 uL", "250 uL"))  

```

This plot suggests that for samples of weight lower than 20 mg, adding a spike lowers the binding deviation. This effect is lost if samples are heaver than 20 mg. 

```{r dev. scatter, message=FALSE, echo=FALSE, fig.height = 4.5, fig.width = 7}

# Scatter plot of Binding deviation vs Weight, 2 groups (by dilution)

ggplot(data, aes(x = Weight_mg, 
                 y = Binding_deviation, 
                 shape = factor(Buffer_nl))) +
  geom_point(size = 3, color = "gray40") +
  geom_smooth(method = "lm", se = FALSE, linewidth = 0.5, color = "orange2") +  # Add a trend line
   labs(title = "Binding Percentage vs Weight, 2 groups (by spike)",
       x = "Weight (mg)", y = "Binding Deviation",
       shape = "Buffer Amount (nl)") +
  scale_y_continuous(n.breaks = 10) +  
  scale_shape_discrete(labels = c("60 uL", "250 uL")) +
  facet_grid( ~ Spike, labeller = as_labeller(c("0" = "Not spiked", "1" = "Spiked"))) 

```

We observe that spiked samples have a higher deviation from the ideal binding. We also observe that having larger samples leads to values closer to 50%. It is interesting to see that error does not go below 15% if we look at samples with weight under 20mg. Yet, we know that a deviation of up to 30% is acceptable. 

```{r boxplot_deviation, message=FALSE, echo = FALSE}

ggplot(data, aes(x = factor(Spike), 
                 y = Binding_deviation, 
                 fill = factor(Spike))) +
  geom_boxplot() +
  labs(title = "Binding deviation, by dilution and spike",
       x = "", 
       y = "Binding deviation") +
  scale_y_continuous(n.breaks = 10) +  
  theme(legend.position = "none") +
  facet_wrap(~ Buffer_nl,  
             labeller = as_labeller(c("0" = "60 uL", 
                                      "1" = "250 uL"))) +
    scale_x_discrete(labels = c("Not spiked", "Spiked")) 
  
```

Here we see how the lowest (best) scores are obtained by the non-spiked groups. Even better results are obtained if the dilution is 250 uL. 

- **Conclusion**: using a **250 uL dilution, without spikes**, will lead to **better results** that fall in the middle of the curve, and allow for more precise calculations of cortisol concentration. 

# Analysis: linear models

To explore the effects of each variable more systematically, I run multiple models and compared them using AIC Akakikes' coefficient. I removed samples with a binding over 80% or under 20%.

```{r qqplot, echo = FALSE}

# remove samples w/ binding higher than 80%

d <- data[is.na(data$Failed_samples), ]
data <- d  

weight <- data$Weight_mg
binding <- data$Binding.Perc
x <- mean(binding)


# Q-Q Plot
qqnorm(binding, pch = 16, col = "gray40")
qqline(binding, col = "orange3", lwd = 1.5)

```
First, I looked at the distribution of the data (binding percentage). I am not sure how to describe it, but it does not look very linear. I will test different distributions at another time, but for now, I will run and compare simple models that should allow me to understand which variables have a greater impact on binding percentages.

```{r models dif weights, echo = FALSE, include = FALSE}
mod <- lm(binding ~ weight)
summary(mod)

plot(weight, binding,
     main = "Linear regression ~ weight",
     xlab = "Weight",
     ylab = "binding %",
     pch = 16,
     cex = 1.3, 
     col = "gray40")

abline(mod, col = "orange3", lwd = 1.5)

# 20
d<- data[c(data$Weight_mg >= 20 & data$Weight_mg <= 29.9), ]
weight <- d$Weight_mg
binding <- d$Binding.Perc

# Q-Q Plot
qqnorm(binding, pch = 16, col = "cyan3")
qqline(binding, col = "red", lwd = 1.5)

## Linear regression
mod <- lm(binding ~ weight)
summary(mod)

plot(weight, binding,
     main = "Linear regression, weight 20 - 29.9 mg",
     xlab = "weight",
     ylab = "binding %",
     pch = 8,
     cex = 1, 
     col = "darkgreen")

abline(mod, col = "red", lty = 'dashed')

# 30

## Linear regression, by weight group
d<- data[c(data$Weight_mg > 29.9), ]
weight <- d$Weight_mg
binding <- d$Binding.Perc

# Q-Q Plot
qqnorm(binding, pch = 16, col = "cyan3")
qqline(binding, col = "red", lwd = 1.5)

## Linear regression
mod <- lm(binding ~ weight)
summary(mod)

plot(weight, binding,
     main = "Linear regression, weight over 30 mg",
     xlab = "weight",
     ylab = "binding %",
     pch = 8,
     cex = 1, 
     col = "darkgreen")

abline(mod, col = "red", lty = 'dashed')
```

```{r models dif spike, include = FALSE}
## Linear regression, by spike
## YES
d<- data[c(data$Spike == 1), ]
weight <- d$Weight_mg
binding <- d$Binding.Perc

# Q-Q Plot
qqnorm(binding, pch = 16, col = "cyan3", main = "qqplot, spiked samples")
qqline(binding, col = "red", lwd = 1.5)

## Linear regression
mod <- lm(binding ~ weight)
summary(mod)

plot(weight, binding,
     main = "Linear regression, spiked",
     xlab = "weight",
     ylab = "binding %",
     pch = 16,
     cex = 1, 
     col = "darkgreen")

abline(mod, col = "red", lty = 'dashed')


## Linear regression, by spike
## NO

d<- data[c(data$Spike == 0), ]
weight <- d$Weight_mg
binding <- d$Binding.Perc

# Q-Q Plot
qqnorm(binding, pch = 16, col = "cyan3", main = "qqplot, NONspiked samples")
qqline(binding, col = "red", lwd = 1.5)

## Linear regression
mod <- lm(binding ~ weight)
summary(mod)

plot(weight, binding,
     main = "Linear regression, not spiked",
     xlab = "weight",
     ylab = "binding %",
     pch = 16,
     cex = 1, 
     col = "darkgreen")

abline(mod, col = "red", lwd = 1.5)


```

## Comparing models

```{r function to extract coefs}

# creating function to extract coeffs
extract_coefs <- function(model, model_name) {
  # Extract summary of the model
  coef_summary <- summary(model)$coefficients
  
  # Create a data frame with term names, estimates, and standard errors
  coef_df <- data.frame(
    term = rownames(coef_summary),
    estimate = coef_summary[, "Estimate"],
    std.error = coef_summary[, "Std. Error"],
    model = model_name  # Add the model name as a new column
  )
  
  # Return the data frame
  return(coef_df)
}
```

```{r create simple linear models}
binding <- data$Binding.Perc
weight <- data$Weight_mg
spike <- data$Spike
buffer <- data$Buffer_nl

# model 1

m1 <- lm(binding ~ weight)
summary(m1)
confint(m1, level = 0.95)

# model 2

m2 <- lm(binding ~ spike)
summary(m2)

confint(m2, level = 0.95)

# model 3

m3 <- lm(binding ~ buffer)
summary(m3)

confint(m3, level = 0.95)

# model 4

m4 <- lm(binding ~ weight + spike)
summary(m4)

confint(m4, level = 0.95)

# model 5 

m5 <- lm(binding ~ weight + buffer)
summary(m5)

confint(m5, level = 0.95)

# model 6

m6 <- lm(binding ~ spike + buffer)
summary(m6)

confint(m6, level = 0.95)

# model 7

m7 <- lm(binding ~ weight + buffer + spike)
summary(m7)

confint(m7, level = 0.95)

# model 8

sp1 <- data[data$Spike == 1,]
sp0 <- data[data$Spike == 0,]

binding1 <- sp1$Binding.Perc
weight1 <- sp1$Weight_mg
spike1 <- sp1$Spike
buffer1 <- sp1$Buffer_nl

m8 <- lm(binding1 ~  buffer1 + weight1)
summary(m8)

# model 9
binding0 <- sp0$Binding.Perc
weight0 <- sp0$Weight_mg
spike0 <- sp0$Spike
buffer0 <- sp0$Buffer_nl

m9 <- lm(binding0 ~  buffer0 + weight0)
summary(m9)

coef_df1 <- extract_coefs(m1, "Model 1")
coef_df2 <- extract_coefs(m2, "Model 2")
coef_df3 <- extract_coefs(m3, "Model 3")
coef_df4 <- extract_coefs(m4, "Model 4")
coef_df5 <- extract_coefs(m5, "Model 5")
coef_df6 <- extract_coefs(m6, "Model 6")
coef_df7 <- extract_coefs(m7, "Model 7")
coef_df8 <- extract_coefs(m8, "Model 8")
coef_df9 <- extract_coefs(m9, "Model 9")

# Combine the data frames for plotting
coef_df <- rbind(coef_df1, coef_df2, coef_df3, coef_df4)

```

##  Plot regression coefs 
```{r Plot reg coefs, echo = FALSE, include = FALSE}

ggplot(coef_df, aes(x = term, y = estimate, color = model)) +
  geom_point(position = position_dodge(width = 0.5)) +  # Points for the estimates
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error),
                position = position_dodge(width = 0.5), width = 0.2) +  # Error bars for confidence intervals
  theme_minimal() +
  coord_flip() +  # Flip the coordinates for better readability
  labs(title = "Coefficient Plot for Multiple Models",
       x = "Terms",
       y = "Estimates") +
  geom_hline(yintercept = 0, color = "gray", linetype = "dashed") +  # Gray line at zero
  theme(legend.position = "bottom")

```

### Plot model 1 to 4

```{r Plot model 1 to 4, fig.height = 6, fig.width = 6 }

ggplot(coef_df, aes(x = term, y = estimate, color = model)) +
  geom_point(position = position_dodge(width = 4)) +  # Points for the estimates
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error),
                position = position_dodge(width = 0.85), width = 1) +  # Error bars for confidence intervals
  theme_minimal() +
  coord_flip() +  # Flip the coordinates for better readability
  facet_wrap(~ model, ncol = 1) +  # One model per line
  labs(title = "Coefficient Plot for Models 1-4",
       x = "Terms",
       y = "Estimates") +
  theme(legend.position = "none") +
  geom_hline(yintercept = 0, color = "gray", linetype = "dashed") +  # Gray line at zero
  expand_limits(y = c(-58, 58)) +
  theme(
    axis.text.x = element_text(size = 12),        # X-axis text size
    axis.text.y = element_text(size = 12),        # Y-axis text size
    axis.title.x = element_text(size = 14),       # X-axis title size
    axis.title.y = element_text(size = 14),       # Y-axis title size
    plot.title = element_text(size = 16, hjust = 0.5),  # Plot title size and centering
    strip.text = element_text(size = 14)          # Facet label text size
  )
  

```

### Plot model 5 to 9

```{r Plot model 5 to 9, fig.height = 8.5, fig.width = 7 }
# Combine the data frames for plotting
coef_df <- rbind(coef_df5, coef_df6, coef_df7, coef_df8,coef_df9)

ggplot(coef_df, aes(x = term, y = estimate, color = model)) +
  geom_point(position = position_dodge(width = 3)) +  # Points for the estimates
  geom_errorbar(aes(ymin = estimate - 1.96 * std.error, ymax = estimate + 1.96 * std.error),
                position = position_dodge(width = 0.9), width = 0.85) +  # Error bars for confidence intervals
  theme_minimal() +
  coord_flip() +  # Flip the coordinates for better readability
  facet_wrap(~ model, ncol = 1) +  # One model per line
  labs(title = "Coefficient Plot for Model 5 to 9",
       x = "Terms",
       y = "Estimates") +
  theme(legend.position = "none") +
  geom_hline(yintercept = 0, color = "gray", linetype = "dashed") +  # Gray line at zero
  expand_limits(y = c(-60, 60)) +
  theme(
    axis.text.x = element_text(size = 12),        # X-axis text size
    axis.text.y = element_text(size = 12),        # Y-axis text size
    axis.title.x = element_text(size = 14),       # X-axis title size
    axis.title.y = element_text(size = 14),       # Y-axis title size
    plot.title = element_text(size = 16, hjust = 0.5),  # Plot title size and centering
    strip.text = element_text(size = 14)          # Facet label text size
  )



``` 

## Summarize info multiple models
```{r}

model_names <- paste("m", 1:9, sep="")
r_values <- 1:9
all_models <- list(m1, m2, m3, m4, m5, m6, m7, m8, m9)
model_info <- c("weight", "spike", "buffer", "weight + spike", "weight + buffer", "spike + buffer", "spike + buffer + weight", "buffer + weight, spiked only","buffer + weight, NOT spiked only")
sum_models <- as.data.frame(r_values, row.names=model_names)
sum_models$res_std_error <- 1:length(model_names)
sum_models$info <- model_info

for (i in 1:length(model_names)) {
    sum_models$r_values[i]      <- summary(all_models[[i]])$adj.r.squared
    sum_models$res_std_error[i] <- summary(all_models[[i]])$sigma
}

kable(sum_models[order(sum_models$r_values, decreasing = TRUE), ]) 
```

```{r coef tab function, include = FALSE}

coeftab <- function(... ,
                    se = FALSE ,
                    se.inside = FALSE ,
                    digits = 4) {
 
    # coeftab - pretty table of coefficients for comparing models
    # works for lm() and a few other object types (e.g., most results of mle2())
    # tends to break when R constructs dummy variables from factors
    # can be avoided by explicitly making dummy variables  before fitting
    
    # initial code written by Richard McElreath (2009)
    # minor updates by Andrew Marshall (2019)
    
    # se = TRUE outputs standard errors
    # se.inside = TRUE prints standard errors in 
    #             parentheses in same column as estimates
    
    # retrieve list of models
    L <- list(...)
    if (is.list(L[[1]]) && length(L) == 1)
        L <- L[[1]]
    
    # retrieve model names from function call
    mnames <- match.call()
    mnames <- as.character(mnames)[2:(length(L) + 1)]
    
    # count number of unique parameters
    param.names <- {
    }
    for (i in 1:length(L)) {
        c.names <- names(coef(L[[i]]))
        param.names <- unique(c(param.names , c.names))
    }
    # columns for standard errors
    if (se == TRUE && se.inside == FALSE) {
        for (i in 1:length(L)) {
            kse.names <- paste(names(coef(L[[i]])) , ".se" , sep = "")
            param.names <- unique(c(param.names , kse.names))
        }
    }
    
    # make empty table
    nk <- length(param.names)
    d <- matrix(NA , ncol = nk)
    d <- data.frame(d)
    colnames(d) <- c(param.names)
    
    # loop over models and insert values
    for (i in 1:length(L)) {
        klist <- coef(L[[i]])
        for (j in 1:length(klist)) {
            d[i, ][names(klist[j])] <- as.numeric(klist[j])
        }
    }
    # insert standard errors
    if (se == TRUE) {
        for (i in 1:length(L)) {
            kse <- sqrt(diag (vcov(L[[i]])))
            for (j in 1:length(kse)) {
                if (se.inside == FALSE)
                    # own column
                    d[i, ][paste(names(kse)[j], ".se", sep = "")] <-
                        as.numeric(kse[j])
                else
                    # combine with estimate
                    d[i, ][names(kse)[j]] <-
                        paste(
                            formatC(as.real(d[i, ][names(kse)[j]]) , digits = digits) ,
                            " (" ,
                            formatC(as.real(kse[j]) , digits = digits) ,
                            ")" ,
                            sep = ""
                        )
            }
        }
    }
    
    # add model names to rows
    rownames(d) <- mnames
    
    # formatting for parenthetical standard errors
    if (se.inside == TRUE && se == TRUE) {
        comment(d) <-
            "Values in parentheses are quadratic estimate standard errors."
        colnames(d) <- paste(colnames(d) , "(se)")
        for (i in 1:nrow(d)) {
            for (j in 1:ncol(d)) {
                d[i, j] <- ifelse(is.na(d[i, j]) , "" , d[i, j])
            }
        }
    }
    
    # return table
    d
}


```

### Comparing models using Akakike's information criteria 

```{r AICc, fig.height = 8, fig.width = 7}

# computing bias-adjusted version of AIC (AICc) Akakaike's information criteria table
AICc_compare <-AICtab(m1, m2, m3, m4, m5, m6, m7, m8, m9, 
        base = TRUE,
        weights = TRUE,
        logLik  = TRUE,
        #indicate number of observations
        nobs = 30)
kable(AICc_compare)

# Coef table 
coeftab(m1, m2, m3, m4, m5, m6, m7, m8, m9) -> coeftabs
kable(coeftabs)

par(mfrow = c(3, 3))

plot(m1, which = 1)  
plot(m2, which = 1, main = "m2")  
plot(m3, which = 1, main = "m3")  
plot(m4, which = 1, main = "m4")
plot(m5, which = 1, main = "m5") 
plot(m6, which = 1, main = "m6")
plot(m7, which = 1, main = "m7")
plot(m8, which = 1, main = "m8")
plot(m9, which = 1, main = "m9")

model <- list(m1, m2, m3, m4, m5, m6, m7, m8, m9)

par(mfrow = c(3, 3))

for (i in 1:length(model)) {
  # Create a Q-Q plot for the residuals of the i-th model
  qqnorm(residuals(model[[i]]), main = paste("Q-Q Plot, m", i, sep = ""))
  qqline(residuals(model[[i]]), col = "red")
}
```

*Model 7 has the highest weight, a measure of certainty in the model*. However, we need to consider that the distribution of the data is not normal. Perhaps I should try using other distributions (binom, posson, )


```{r Models mle2 and dif distributions}

#scale variable

d2 <- data
d2$y <- data$Binding.Perc/100 

nll_beta <- function(mu, phi) {
  a <- mu * phi
  b <- (1 - mu) * phi
  -sum(dbeta(d2$y, a, b, log = TRUE))
}

# Fit models using mle2

fit <- mle2(nll_beta, start = list(mu = 0.5, phi = 1), data = d2)
summary(fit)

m0n <- mle2(d2$y ~ dnorm(mean = a, sd = sd(d2$y)), start = list(a = mean(d2$y)), data = d2) 

# percent cover as predictor, use normal distribution
mcn <- mle2(d2$y ~ dnorm(mean = a + b * d2$Weight_mg, sd = sd(d2$y)), start = list(a = mean(d2$y), b = 0, s = sd(d2$y)), data = d2)

# scatter plot of 

plot(d2$y ~ d2$Weight_mg, 
     xlab = "Buffer",
     ylab = "% binding",
     col = "salmon",
     pch = 16, 
     las = 1)

#m0n
k <-coef(m0n)
curve(k[1] + 0 * x, 
      from = 0, to = 100, 
      add=T, lwd = 3, 
      col = "black")

#mcn
k <-coef(mcn)
curve(k[1] + k[2] * x, 
      from = 0, to = 100, 
      add=T, lwd = 2, 
      col = "lightgreen", 
      lty = "dashed")

```

### Finding optimal parameters using model 7
The goal is to run essays that result in a 50% binding.
```{r Find optimal parameters}

# choose one model (m7: buffer + weight + spike)
coef <- coef(m7)

# Set target binding
target_binding <- 50

# FUNCTION to Solve for weight, assuming spike = 0
# 50% - intercept - (buffer1 * 1) - (spike * 0) / weight  

solve_for_weight <- function(dilution_value, spike_value = 0) {
  (target_binding - coef[1] - coef[3] * dilution_value - coef[4] * spike_value) / coef[2]
}

# Find the weight that gives 50% binding whenspike is 0
# dilution = 250
optimal_weight <- solve_for_weight(dilution_value = 1)
optimal_weight
# dilution = 60
optimal_weight <- solve_for_weight(dilution_value = 0)
optimal_weight

# Find the weight that gives 50% binding when spike is 1
# dilution = 250
optimal_weight <- solve_for_weight(dilution_value = 1, spike_value = 1)
optimal_weight
# dilution = 60
optimal_weight <- solve_for_weight(dilution_value = 0, spike_value = 1)
optimal_weight


# visualize results
## Buffer = 1, Spike = 0
new_data <- expand.grid(weight = seq(min(weight), max(weight), length.out = 100),
                        buffer = as.factor(1), spike = as.factor(0))  # Set buffer and spike to a fixed value for simplicity

# Predict the binding percentage for the new data
new_data$predicted_binding <- predict(m7, newdata = new_data)

# Plot the predicted binding percentage against weight
ggplot(new_data, aes(x = weight, y = predicted_binding)) +
  geom_line() +
  geom_hline(yintercept = 50, linetype = "dashed", color = "red") +  # Highlight 50% binding
  labs(title = "Predicted Binding Percentage vs Weight (dilution = 60, spike = 1)",
       x = "Weight",
       y = "Predicted Binding Percentage")

## Buffer = 1, Spike = 1
new_data <- expand.grid(weight = seq(min(weight), max(weight), length.out = 100),
                        buffer = as.factor(1), spike = as.factor(1))  # Set buffer and spike to a fixed value for simplicity

# Predict the binding percentage for the new data
new_data$predicted_binding <- predict(m7, newdata = new_data)

# Plot the predicted binding percentage against weight
ggplot(new_data, aes(x = weight, y = predicted_binding)) +
  geom_line() +
  geom_hline(yintercept = 50, linetype = "dashed", color = "red") +  # Highlight 50% binding
  labs(title = "Predicted Binding Percentage vs Weight (dilution = 250, spike = 1)",
       x = "Weight",
       y = "Predicted Binding Percentage")


## Buffer = 0, Spike = 1

new_data <- expand.grid(weight = seq(min(weight), max(weight), length.out = 100),
                        buffer = as.factor(0), spike = as.factor(1))  # Set buffer and spike to a fixed value for simplicity

# Predict the binding percentage for the new data
new_data$predicted_binding <- predict(m7, newdata = new_data)

# Plot the predicted binding percentage against weight
ggplot(new_data, aes(x = weight, y = predicted_binding)) +
  geom_line() +
  geom_hline(yintercept = 50, linetype = "dashed", color = "red") +  # Highlight 50% binding
  labs(title = "Predicted Binding Percentage vs Weight (dilution = 250, spike = 0)",
       x = "Weight",
       y = "Predicted Binding Percentage")

## Buffer = 0, Spike = 0

new_data <- expand.grid(weight = seq(min(weight), max(weight), length.out = 100),
                        buffer = as.factor(0), spike = as.factor(0))  # Set buffer and spike to a fixed value for simplicity

# Predict the binding percentage for the new data
new_data$predicted_binding <- predict(m7, newdata = new_data)

# Plot the predicted binding percentage against weight
ggplot(new_data, aes(x = weight, y = predicted_binding)) +
  geom_line() +
  geom_hline(yintercept = 50, linetype = "dashed", color = "red") +  # Highlight 50% binding
  labs(title = "Predicted Binding Percentage vs Weight (dilution = 60, spike = 0)",
       x = "Weight",
       y = "Predicted Binding Percentage")



# FUNCTION to Solve for weight, assuming spike = 0
# 50% - intercept - (buffer1 * 1) - (spike * 0) / weight  

solve_for_weight <- function(dilution_value, spike_value = 0) {
  (target_binding - coef[1] - coef[3] * dilution_value - coef[4] * spike_value) / coef[2]
}


# Loop over different dilution values to find the optimal weight for 50% binding
# here, dilution 0 means 60uL, and 1 is 250 uL
for (buffer in seq(0, 1, by = 0.1)) {
  optimal_weight <- solve_for_weight(buffer)
  cat("Dilution:", buffer, "-> Optimal Weight for 50% Binding:", optimal_weight, "\n")
}

```



```{r echo = FALSE}


#wflow_publish("./analysis/ELISA_visualizations.Rmd")

#wflow_status()


```


